{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sghosh32/Word2Vec/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCYEYJucVNKM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import cm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"vocabulary is basically a list of unique words with assigned indices. corpus is very simple and short. warsaw is poland capital. berlin is germany\n",
        " capital. paris is france capital. the paper was eventually banned. the future of the bears was under discussion. he had a medal. he is a king. she is a \n",
        " queen. all the roads going into the town were gravelled. he drove off the road. i need you to sign these papers. we drink coffee every morning.\"\"\".split()\n",
        "\n",
        "#print(corpus)"
      ],
      "metadata": {
        "id": "Hu-WC-1gw71A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_context_vector(context, word_to_ix, vocab_size):\n",
        "  idxs = [word_to_ix[w] for w in context]\n",
        "  return torch.tensor(idxs, dtype = torch.long)"
      ],
      "metadata": {
        "id": "LZ2mENmtKFTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 2\n",
        "embedding_dimension = 100"
      ],
      "metadata": {
        "id": "tSJ91ke6LuGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = \" \".join(corpus).split()\n",
        "print(vocab)\n",
        "vocab = list(set(vocab))\n",
        "print(vocab)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "wLD_GBp26t9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fd2ef1-0f7a-4973-9cb1-0d09d79c91e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vocabulary', 'is', 'basically', 'a', 'list', 'of', 'unique', 'words', 'with', 'assigned', 'indices.', 'corpus', 'is', 'very', 'simple', 'and', 'short.', 'warsaw', 'is', 'poland', 'capital.', 'berlin', 'is', 'germany', 'capital.', 'paris', 'is', 'france', 'capital.', 'the', 'paper', 'was', 'eventually', 'banned.', 'the', 'future', 'of', 'the', 'bears', 'was', 'under', 'discussion.', 'he', 'had', 'a', 'medal.', 'he', 'is', 'a', 'king.', 'she', 'is', 'a', 'queen.', 'all', 'the', 'roads', 'going', 'into', 'the', 'town', 'were', 'gravelled.', 'he', 'drove', 'off', 'the', 'road.', 'i', 'need', 'you', 'to', 'sign', 'these', 'papers.', 'we', 'drink', 'coffee', 'every', 'morning.']\n",
            "['a', 'corpus', 'bears', 'simple', 'basically', 'he', 'town', 'these', 'unique', 'and', 'roads', 'france', 'papers.', 'morning.', 'she', 'king.', 'capital.', 'words', 'coffee', 'the', 'list', 'is', 'germany', 'paris', 'into', 'paper', 'warsaw', 'drove', 'very', 'queen.', 'need', 'under', 'discussion.', 'indices.', 'i', 'eventually', 'of', 'we', 'banned.', 'vocabulary', 'off', 'with', 'drink', 'berlin', 'poland', 'medal.', 'were', 'all', 'every', 'was', 'to', 'short.', 'going', 'sign', 'future', 'assigned', 'gravelled.', 'you', 'had', 'road.']\n",
            "60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "print(ix_to_word) \n",
        "#print(len(corpus))"
      ],
      "metadata": {
        "id": "pel6LjsN73Ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd411a0-121f-44a2-c51b-7976cdf99e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'a', 1: 'corpus', 2: 'bears', 3: 'simple', 4: 'basically', 5: 'he', 6: 'town', 7: 'these', 8: 'unique', 9: 'and', 10: 'roads', 11: 'france', 12: 'papers.', 13: 'morning.', 14: 'she', 15: 'king.', 16: 'capital.', 17: 'words', 18: 'coffee', 19: 'the', 20: 'list', 21: 'is', 22: 'germany', 23: 'paris', 24: 'into', 25: 'paper', 26: 'warsaw', 27: 'drove', 28: 'very', 29: 'queen.', 30: 'need', 31: 'under', 32: 'discussion.', 33: 'indices.', 34: 'i', 35: 'eventually', 36: 'of', 37: 'we', 38: 'banned.', 39: 'vocabulary', 40: 'off', 41: 'with', 42: 'drink', 43: 'berlin', 44: 'poland', 45: 'medal.', 46: 'were', 47: 'all', 48: 'every', 49: 'was', 50: 'to', 51: 'short.', 52: 'going', 53: 'sign', 54: 'future', 55: 'assigned', 56: 'gravelled.', 57: 'you', 58: 'had', 59: 'road.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in range(context_size, len(corpus) - context_size):\n",
        "  context = [corpus[i - 1], corpus[i + 1]]\n",
        "  target = corpus[i]\n",
        "  data.append((context, target))\n",
        "print(data[0])"
      ],
      "metadata": {
        "id": "bLQmowdMM41J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c64f345-f11d-4730-d474-fd8fa5dc50e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['is', 'a'], 'basically')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dimension):\n",
        "    super(Word2Vec, self).__init__()\n",
        "\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dimension)\n",
        "    self.hidden1 = nn.Linear(embedding_dimension, 128)\n",
        "    self.activation_function1 = nn.Tanh()\n",
        "\n",
        "    self.hidden2 = nn.Linear(128, vocab_size)\n",
        "    self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    word_embeddings = sum(self.embeddings(x)).view(1, -1)\n",
        "    output = self.hidden1(word_embeddings)\n",
        "    output = self.activation_function1(output)\n",
        "    output = self.hidden2(output)\n",
        "    output = self.activation_function2(output) \n",
        "    return output    \n",
        "\n",
        "  def get_word_embedding(self, word):\n",
        "    word = torch.tensor([word_to_ix[word]])\n",
        "    return self.embeddings(word).view(1, -1)"
      ],
      "metadata": {
        "id": "N_JASahyFOjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(vocab_size, embedding_dimension)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimize = optim.Adam(model.parameters(), lr = 0.005)"
      ],
      "metadata": {
        "id": "tjjBpFJBY8sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(500):\n",
        "  loss = 0\n",
        "  loss_list = []\n",
        "  for context, target in data:\n",
        "    context_vector = make_context_vector(context, word_to_ix, vocab_size)\n",
        "    #print(context_vector)\n",
        "    log_probs = model(context_vector)\n",
        "    #print(log_probs)\n",
        "    loss += criterion(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "  if (epoch + 1) % 100 == 0:\n",
        "    print('Epoch: ', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "  optimize.zero_grad()\n",
        "  loss.backward()\n",
        "  optimize.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cUqV7qosvxH",
        "outputId": "eba08c13-b696-4dc8-ba84-a3f31e414c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0100 cost = 17.008297\n",
            "Epoch:  0200 cost = 16.950745\n",
            "Epoch:  0300 cost = 16.926922\n",
            "Epoch:  0400 cost = 16.914736\n",
            "Epoch:  0500 cost = 16.907621\n"
          ]
        }
      ]
    }
  ]
}